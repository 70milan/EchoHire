{
  "name": "M",
  "target_positions": [
    "Data Engineer"
  ],
  "summary": "Experienced data engineer ",
  "skills": [
    "na"
  ],
  "resume_text": "Milan Barot\n\nMilanbarot6979@gmail.com - +1(917) 497-6979  -  Fairfax, VA  -  www.linkedin.com/in/mb70\n\nProfessional Summary:\n\nAccomplished Data Engineer with 9 years of experience in architecting, developing, and optimizing end-to-end data integration, analytics, and modernization solutions across healthcare, real estate, and financial domains. Proven expertise in transitioning legacy ETL systems to modern cloud-native architectures leveraging Azure Data Factory (ADF), Azure Databricks (PySpark, Delta Lake), and SQL Server, with hands-on experience across all phases of the data lifecycle — from ingestion, transformation, and validation to visualization and delivery.\n\nAt Blue Cross Blue Shield of Arizona (via Mastek), led large-scale system modernization initiatives, migrating legacy SSIS workflows into scalable ADF pipelines that support HIPAA-compliant processing of high-volume eligibility, claims, accumulator, and value-based care data. Rebuilt and restored critical ETL pipelines post–CHC system hack (2024) under aggressive recovery timelines, ensuring uninterrupted data delivery and regulatory compliance. Spearheaded the design and implementation of the Supplemental 837 ETL solution—automating ingestion, transformation, and secure generation of compliant EDI 837 claim files integrated with HealthRules Payer (HRP) and vendor systems such as OptumRx, HealthEquity, and Equality Health.\n\nAt CBRE, played a pivotal role in the Data Engineering & Reporting team during the COVID-19 pandemic, modernizing legacy PostgreSQL-based analytics into scalable Python, PySpark, and Airflow-driven ETL pipelines using CBRE’s internal framework (Lymbic). Engineered automated data ingestion and transformation workflows that integrated external datasets like Google Mobility and BREAM assessments into Tableau dashboards, enabling leadership to track building utilization, leasing risk, and tenant behavior in real time. Contributed to the migration of analytical workloads from PostgreSQL to Apache Hive, leveraging PySpark for distributed processing and improving performance and scalability for global advisory datasets.\n\nEarlier, at National Mortgage Insurance Corporation (National MI), spearheaded the development of a mortgage data integration and reporting system (MDIPRS) — building parameterized ADF pipelines, dimensional data models, and partner-compliant EDI/MISMO data feeds. Delivered compliant mortgage datasets to major financial institutions such as Wells Fargo and Bank of America, while automating data quality checks, encryption, and governance for regulatory adherence.\n\nStarted career as a SQL Server Developer at Webmobi Technologies, mastering T-SQL, SSIS, SSRS, and data modeling (ERwin) to build ETL processes, automate workflows, and create BI reports using Power BI and Tableau. Developed strong foundations in database design, data warehousing, and visualization, establishing the groundwork for advanced data engineering leadership in cloud and big data ecosystems.\n\nAcross all projects, consistently recognized for scalable design thinking, cross-functional collaboration, and data quality excellence — enabling organizations to transition seamlessly from on-prem to cloud ecosystems, reduce manual intervention, and unlock faster, more reliable, and compliant data delivery.\n\nCore Competencies\n\nETL Development & Modernization: ADF, SSIS, Databricks, Airflow\n\nBig Data & Cloud Platforms: Azure, AWS, Snowflake, Delta Lake, ADLS\n\nProgramming & Scripting: PySpark, SQL, Python, T-SQL\n\nData Modeling & Warehousing: Star/Snowflake schema, ERwin, Dimensional design\n\nAutomation & DevOps: Azure DevOps, Git, Jenkins, ActiveBatch, CI/CD\n\nVisualization & Reporting: Power BI, Tableau, SSRS\n\nDomain Expertise: Healthcare (eligibility, claims, VBC), Real Estate, Mortgage Insurance\n\nGovernance & Compliance: HIPAA, GLBA, MISMO standards\n\nKey Achievements:\n\nReduced data delivery turnaround for financial and trading partners through optimized ETL pipelines, ensuring timely and accurate data exchange.\n\nImproved data processing efficiency by 35–50% via automation, incremental loading, and SQL optimization across high-volume healthcare data pipelines.\n\nAchieved full audit compliance during internal and external reviews, strengthening partner confidence and ensuring strict HIPAA and CMS regulatory adherence.\n\nDelivered Supplemental 837 claim files to federal and state agencies such as CMS, automating validation, formatting, and secure transmission processes in compliance with payer standards.\n\nExecuted rapid ETL restoration post–CHC system hack (2024)—a mission-critical recovery that reinstated eligibility and claims data pipelines under aggressive timelines with near-zero data loss and minimal downtime, safeguarding core business operations.\n\nEnabled real-time decision-making for executive leadership through centralized analytics platforms and validated datasets during pandemic-driven disruptions.\n\nAutomated manual workflows using SQL Server Agent, ActiveBatch, and FTP delivery mechanisms, improving operational efficiency and ensuring SLA compliance across partner integrations.\n\nTECHNICAL SKILLS:\n\nProfessional Experience: -\n\nETL Developer/Data engineer - Mastek for Blue Cross Blue Shield - Phoenix, AZ (Healthcare/ insurance)                 \n\nSep 22 – current\n\nkEY ROLES AND RESPONSBILITIES: \n\nProject: Trading Partner Data Management; System Modernization, CHC Hack remediation \n\nLed the design and implementation of end-to-end ETL pipelines to manage eligibility, claims, and value-based care data with key trading partners. Automated data workflows and migration from legacy systems to HealthRules Payer, ensuring seamless integration and compliance. \n\nDesigned, developed, and maintained ETL pipelines using Azure Data Factory (ADF) and SSIS to extract, transform, and load large-scale healthcare data from on-prem SQL Server into Azure SQL and Delta Lake environments. Ensured HIPAA compliance and vendor-specific data quality standards across all data flows.\n\nMigrated legacy SSIS workflows to ADF with parameterized and metadata-driven orchestration, automating data extraction from on-prem SQL servers, staging in Azure Blob Storage, and transformation using SQL Scripts for downstream analytics and reporting.\n\nDesigned and implemented end-to-end ETL pipelines using SSIS and SQL Server to process eligibility, claims, accumulator, and value-based care (VBC) files, ensuring compliance with business requirements and vendor-specific standards. Collaborated with vendors such as OptumRx, HealthEquity, Equality Health, and others to deliver high-quality data solutions.\n\nOversaw a small team of offshore resources by recruiting and onboarding team members as required for project execution. Ensured tasks were completed on time by maintaining close collaboration with both onshore and offshore teams \n\nUtilized Azure Data Factory (ADF) to design and orchestrate scalable data integration pipelines for migrating legacy member and claims data to HealthRules Payer (HRP) on Azure Cloud.\n\nLeveraged Azure SQL Database and Azure Data Lake Storage for staging, transformation, and persistent storage of high-volume healthcare data. Configured ADF linked services, pipelines, and triggers for automated, scheduled data ingestion from on-premises SQL Server to Azure.\n\nRebuilt critical ETL pipelines using SSIS and SQL Server after the CHC system hack (2024), restoring high-priority data workflows under tight deadlines with minimal downtime.\n\nDeveloped complex data extracts that combined on-prem warehouse data with select cloud-based sources, ensuring consistency, accuracy, and seamless integration across hybrid environments reducing execution time by 25 % on an average across the board.\n\nCollaborated closely with Deloitte on the HealthEquity project to develop eligibility and claims files from scratch. Responsibilities included gathering business requirements, creating technical documentation, optimizing ETL pipelines, conducting performance testing, and ensuring smooth deployment into production environments.\n\nAutomated data workflows by integrating SQL Server Agent jobs with ActiveBatch, setting up FTP connectivity with strict naming conventions and cadence-driven schedules for file delivery to vendors. \n\nDeveloped an ASP.NET web application executing CRUD operations on an on an on-prem OLTP warehouse, eliminating manual data entry and significantly improving data accuracy. \n\nGenerated and processed EDI 834, 837 Enrollment and Dental Enrollment files, facilitating secure data exchange through EDI tools while maintaining robust documentation for all processes to ensure audit readiness.\n\nConducted performance reviews of legacy pipelines, refactoring and optimized code upto 50% for improved scalability and efficiency while adhering to best practices in data engineering. \n\nDesigned and deployed interactive and dynamic dashboards using Microsoft’s SSRS with Drill Down, Drill Through, and Drop-down menu options and parameterized reports, enabling stakeholders to access real-time insights and make data-driven decisions. \n\nPartnered with business stakeholders to gather requirements for new projects, conducting detailed assessments of complexity, technical effort, development timelines, and warranty periods. Worked with Mastek to draft SOWs for approval by Blue Cross Blue Shield, ensuring alignment with business objectives.\n\nParticipated in Agile sprint meetings to track progress on deliverables, resolve blockers, and ensure timely completion of project milestones across multiple initiatives.\n\nBusiness & Technical Impact\n\nRestored critical healthcare data pipelines post–CHC system hack (2024) within days, ensuring business continuity, zero regulatory breaches, and uninterrupted claims processing across high-priority lines of business.\n\nDelivered Supplemental 837 claim files to CMS and other government agencies, implementing automated ETL and secure EDI delivery mechanisms that ensured full compliance with HIPAA and CMS mandates.\n\nReduced manual intervention by 65% through ADF trigger & ActiveBatch-driven automation, enabling consistent vendor data deliveries.\n\nStandardized 30+ vendor data pipelines using a modular and dynamic SQL and Store procedures, significantly cutting down maintenance overhead.\n\nEnvironment: MS SQL Server, SSIS, SSRS, Azure Data Factory (ADF), SQL Server, SSIS, Azure SQL Database, ActiveBatch, Power BI, Azure DevOps, Git\n\ndata engineer - CBRE, Dallas, TX, \t                                                                \t\t\t\t    Nov 20 – Sept 22\n\nKEY ROLES AND Responsibilities:\t\n\nProject: Advisory Datamart Implementation\n\nDesigned and implemented a scalable Data Mart to centralize and integrate multiple data sources, enabling CBRE’s advisory teams to gain actionable insights into property utilization, tenant occupancy, leasing risk, and market trends during the COVID-19 pandemic.\n\nPlayed a key role on the Data Engineering & Reporting team during the COVID-19 pandemic,   delivering real-time data pipelines and analytics that supported CBRE’s Advisory business unit across global commercial real estate portfolios.\n\nEngineered and optimized ETL pipelines primarily in Python, leveraging PostgreSQL for storage and transformation logic within CBRE’s internal Python-based ETL framework (Lymbic). Developed reusable Python modules for data ingestion, transformation, and validation, translating complex business rules from real estate analysts into scalable Python/Pandas workflows integrated with SQL.\n\nBuilt robust Python ingestion scripts to pull diverse external datasets (e.g., Google Mobility Reports, BREAM energy efficiency assessments), applying Pandas for high-performance transformation into Parquet/CSV formats. These processed datasets powered Tableau dashboards for leadership to assess COVID-19’s regional impacts on commercial spaces.\n\nDesigned, developed, and maintained robust ETL pipelines using Python, SQL, and PostgreSQL via CBRE’s internal ETL framework (Lymbic). Translated complex business logic from real estate analysts into SQL/Python transformations for property usage, tenant risk, and leasing performance.\n\nIngested diverse external data sources such as Google Mobility Reports and BREAM energy efficiency assessments, transforming them into structured datasets in Parquet and CSV formats. These outputs fed into Tableau dashboards used by leadership to assess region-wise COVID impacts on commercial spaces.\n\nWorked in an Agile delivery environment, actively collaborating with analysts, product managers, and backend engineers in weekly sprint meetings to prioritize features, track bugs, and refine pipeline outputs based on stakeholder needs.\n\nEnabled high-impact real estate decisions by producing clean, trusted data for dashboards that revealed critical trends such as building underutilization, shifting tenant behaviors, and geographic leasing risk. These insights directly influenced CBRE’s portfolio restructuring strategies and client advisory recommendations during the pandemic.\n\nBusiness & Technical Impact\n\nEnabled real-time analytics for CBRE’s global Advisory business, empowering leadership to make data-driven decisions on portfolio restructuring, tenant risk, and leasing performance during pandemic disruptions.\n\nReduced data processing turnaround by 40% through Python-based ETL optimization, parallelization with PyArrow, and incremental pipeline scheduling in Airflow.\n\nStandardized ingestion and transformation logic for 10+ external datasets (e.g., Google Mobility, BREAM), improving data consistency and enabling cross-regional comparability across commercial real estate portfolios.\n\nImproved stakeholder visibility by integrating validated datasets into Power BI dashboards, reducing manual reporting cycles from days to hours and supporting executive-level risk assessments.\n\nEnvironment:AWS, Python, Pyspark, Jenkins, Json, Postgres, Snowflake, Data Infrastructure Development, Automation, Jenkins, Docker, Python, Jenkins, Technical Documentation\n\nSQL/etl developer National MI, New York, NY\t                                                                                   Oct 18 – Feb 20\n\nKEY ROLES AND Responsibilities:\n\nProject: Mortgage Data Integration and Partner Reporting System (MDIPRS)\nA compliance-driven data reporting platform that enabled secure, timely delivery of mortgage insurance and partner data to external financial institutions, ensuring data governance and business continuity.\n\nArchitected, developed, and maintained Azure Data Factory (ADF) pipelines to orchestrate end-to-end data workflows, enabling reliable extraction of transactional loan and insurance data from on-premises SQL Server OLTP systems into cloud-based Azure SQL Databases optimized for analytics and reporting.\n\nEmployed modular pipeline design patterns with parameterization and metadata-driven configurations, facilitating scalable management of multiple trading partner data delivery processes while reducing development and maintenance overhead.\n\nAutomated data ingestion workflows with incremental load mechanisms using watermark columns and dynamic queries to efficiently process high-volume mortgage and claims datasets on a daily and monthly basis.\n\nCollaborated with cross-functional teams including business analysts, compliance officers, and IT infrastructure teams to gather requirements, validate business rules, and ensure adherence to regulatory standards such as MISMO data format compliance and GLBA for data privacy.\n\nDeveloped comprehensive data validation frameworks leveraging T-SQL stored procedures and dynamic SQL for rigorous quality assurance checks including foreign key integrity, null handling, business rule enforcement, and duplicate record elimination prior to partner file generation.\n\nCreated and optimized a dimensional data warehouse schema implementing star schema principles with dimension tables (Dim_Lender, Dim_Borrower, Dim_Loan, Dim_Property) and fact tables (Fact_Mortgage_Applications, Fact_Claims, Fact_Premiums) to support complex analytical queries and BI reporting.\n\nDesigned and delivered customized, partner-specific data files in formats such as CSV and XML, conforming to MISMO standards, and securely transmitted via SFTP and Azure Blob Storage to financial institutions including Bank of America, SoFi, and Wells Fargo.\n\nBuilt and enhanced interactive Power BI dashboards presenting KPIs like loan approval rates, claim settlement times, premium revenue trends, and partner data delivery SLA compliance, empowering executive leadership and operational teams with real-time actionable insights.\n\nIntegrated robust monitoring and alerting mechanisms using Azure Monitor and custom logging tables to track pipeline execution health, performance metrics, and failure notifications, ensuring prompt incident response and minimal downtime.\n\nSpearheaded documentation efforts including detailed data dictionaries, ETL process flows, and partner file specifications to facilitate knowledge transfer, audit readiness, and onboarding of new team members.\n\nParticipated in quarterly data governance reviews and security audits to enforce best practices for data handling, encryption, access controls, and compliance with industry regulations, significantly reducing risk of data breaches and enhancing partner trust.\n\nProvided ongoing production support, troubleshooting data discrepancies, performance bottlenecks, and working closely with support teams to implement bug fixes, performance tuning, and pipeline enhancements.\n\nEnvironment: SQL Server, SSIS, SSRS, Power BI, ActiveBatch, Python, Azure DevOps, Azure DataFactory, MS Office 365, Agile/Scrum \n\nSQL Server developer  - WebMobi Technologies, Vadodara, IND/New \n\nBrunswick New Jersey\t\t\t                                                                                                               Mar 14 -Oct 18\n\nKEY ROLES AND Responsibilities:\n\nSQL Server 2012 RDBMS database development using T-SQL programming, queries, Stored Procedures, Views. Worked on Extracting, Transforming and Loading (ETL) process to load data from Excel, Flat file to MS SQL Server using SSIS. Created SSIS packages to load the data from Text File to staging server and then from staging server to Data warehouse.\n\nETL implementation using SQL Server Integration Services (SSIS), Applying some business logic and data cleaning in a staging server to maintain child and parent relationship. Worked on Control flow tasks such as Execute SQL task, Send Mail Task, File System Task, Data Flow Task and used different data sources and destination with derived column, lookup transformation within Data Flow Task.\n\nExperience in creating SSIS packages using different type’s tasks, with Error Handling, Log particular event by creating logging and working on package configuration to run packages on different servers. Created Database and Database Objects like Tables, Stored Procedures, Views, Triggers, Rules, Defaults, user defined data types and functions.\n\nAdept at infrastructure as code (IaC) using AWS CloudFormation, automating the provisioning and management of AWS resources for rapid and consistent deployments. Automated repetitive tasks such as data transformation and cleaning, resulting in a 40% reduction in manual effort and improved data consistency.\n\nDeveloped an ASP.NET web application using Framework 6 that performed CRUD operations on a SQL Server database, eliminating the need for manual data entry and a significant increase in data accuracy Troubleshoot and resolve database-related issues, including database connectivity, security, and performance issues. Collaborate with cross-functional teams to gather business requirements, analyse data and provide solutions for business problems.  Develop a server environment where users are placed in a queue and only one user is allowed access to the hardware. \n\nGood understanding and experience in implementation of JavaScript design patterns and frameworks (AngularJS, Angular). Modify the LabView programs suitable to the server program. Wrote and executed various MYSQL database queries from python using Python-MySQL connector and MySQL dB package.\n\nCreated a Power BI data model based on analysis of the end-user workflow data provided by the client. Imported data from SQL Server DB, and Azure SQL DB to Power BI to generate reports. Developed analysis reports and visualization using DAX functions like table, aggregation, and iteration functions. Optimized data collection procedures and generated reports weekly, monthly, and quarterly. Designed, developed, tested, and maintained Tableau functional reports based on client requirements\n\nPresented application to Hospital’s Executive Team and put the application into a production system for them to use SQL & Tableau Projects. Produced custom visualizations for the client to monitor all patient registration-related workflow, thus giving excess to their supervisor team to monitor high error rate departments\n\nExtracted data from SQL Server database tables into flat data files and Excel sheets using table export and BCP for easy data migration. Developed advanced SQL queries with multi-table joins, group functions, subqueries, set operations, and T- SQL stored procedures, user-defined functions (UDFs) for data analysis\n\nInvolved in designing physical and logical data models using the ERwin Data modeling tool. Designed the relational data model for operational data store and staging areas, Designed Dimension & Fact tables for data marts. Extensively used ERwin data modeler to design Logical/Physical Data Models and relational database design. Created Stored Procedures, Database Triggers, Functions, and Packages to manipulate the database and apply the business logic according to the user's specifications.\n\nCreated Triggers, Views, Synonyms, and Roles to maintain integrity plan and database security. Creation of database links to connect to the other server and access the required info. Integrity constraints, database triggers, and indexes were planned and created to maintain data integrity and to facilitate better performance. Used Advanced Querying for exchanging messages and communicating between different modules. System analysis and design for enhancements Testing Forms, Reports and User Interaction.\n\nEnvironment: SQL Server 2012/2016, Server Integration Services 2012/2016, SQL server Reporting Service 2012/2016, Microsoft Visual Studio, SQL Server Management Studio,  MS Excel, T-SQL,ERWIN 7.2, C# .Net, SharePoint , TFS, Unix Scripting, MySQL, Angular JS, Microsoft Power BI, Tableau, SQL, Excel, Agile/Scrum, Oracle 9i, SQL* Plus, PL/SQL, ERwin, TOAD, Stored Procedures.\n\nEducation:\n\nGujarat Technological University\t-2014 \n\nBachelor’s in Computer Applications\t\t\t                                                         \t\t    \n\nCertifications:\n\nAzure Data Fundamentals - 2023 \n\nDatabricks Certified Data Engineer Associate – exp Dec 2025 ",
  "job_description": "asdasd"
}